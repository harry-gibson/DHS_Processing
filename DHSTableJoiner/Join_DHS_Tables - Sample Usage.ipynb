{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DHS recode generator / table joiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains code for producing custom output \"recode\" tables from DHS tables, which have first been processed out into individual files for each survey and table (record type). \n",
    "\n",
    "Essentially it provides a means of \"joining\" tables that are stored in flat CSV files, just like executing a join query on data held in a DB. In fact the joining is done by loading each of the input tables to an in-memory SQLite database, building the output table there, and dumping the output table back to a CSV file.\n",
    "\n",
    "The code for actually constructing the SQL that builds the in-memory database, and performs the joins, is implemented in an external module that should be in the same directory as this notebook. This notebook just contains the necessary code to read the input requirements and data, and write the output files.\n",
    "\n",
    "This was developed by Harry Gibson for extracting information (potentially) related to Under 5 Mortality for Donal Bisanzio. However it should be applicable for creating any \"flat\" joined output tables from DHS data that has been parsed into separate tables from the CSPro format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main input is a list of tables / variables that should go into the output, specified in a CSV file. This has one row for each output column, and it should have the columns \"Name\" and \"RecordName\" which specify the variable name and table name respectively.\n",
    "\n",
    "A second input CSV file provides a list of survey IDs (in a \"DHS_id\" column) that data should be extracted from, or for just a few surveys you could specify these manually in the notebook.\n",
    "\n",
    "Finally a directory path to the parsed DHS survey tables (produced using the DHS survey parsing code) must be provided. The survey id and record name from the above inputs will be used to select CSV files from this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import sqlite3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from DHSTableManagement import *\n",
    "from UnicodeWriter import UnicodeWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Specify input file locations / patterns\n",
    "\n",
    "You need to specify:\n",
    "* varsFile - must contain columns Name, RecordName, and Len, definining the columns and their source tables which should appear in the joined output table\n",
    "* svyFile - must contain a column \"DHS_id\" giving the numeric survey ID for each survey we want to run against, to find the survey files (unless you just specify the survey IDs manually later on)\n",
    "* outDir and outFNPattern - for generating the name and path of the output file for each survey\n",
    "* a list of glob patterns for the survey files, with string format placeholders for the survey id and the table name. If all files are in one folder then this list will contain only one pattern\n",
    "* a list of glob patterns for the survey spec files (record spec), with string format placeholders for the survey id and the table name. If all files are in one folder then this list will contain only one pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infoDir = r'\\\\path\\to\\folder\\with\\specification\\files'\n",
    "infoDir = r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\Processing\\U5M_TheUniverse_And_Everything_201510\\Info'\n",
    "# csv file containing columns for variable names and their source tables required in output\n",
    "# Must have columns Name, RecordName, and Len\n",
    "varsFile = os.path.join(infoDir, 'variables_chosen_with_lengthandtype.csv')\n",
    "# list of numerical survey ids in column DHS_id (unless entering list manually)\n",
    "svyFile = os.path.join(infoDir, 'survey_db_list.csv')\n",
    "\n",
    "outDir = r'\\\\output\\folder\\path'\n",
    "outDir = r'C:\\Temp\\test'\n",
    "\n",
    "# identifier for the output filenames\n",
    "outputFilenameTag = \"U5M_All_Final_Surveys\"\n",
    "outFNPattern = os.path.join(outDir,outputFilenameTag+\".{0!s}.csv\")\n",
    "\n",
    "# Glob pattern(s) for all parsed DHS tables (assuming we're reading from CSVs, not DB)\n",
    "# string format placeholder {0} should be survey number and {1} should be table id name\n",
    "tablePatterns = [\n",
    "    r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\DataExtraction\\20150626_FullSiteScrape\\ProcessedTables_PartTrimmed\\{0}.*.{1}.csv',\n",
    "    r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\DataExtraction\\20160307_Updates\\ProcessedTables_PartTrimmed\\{0}.*.{1}.csv'\n",
    "]\n",
    "# Glob pattern(s) for all FlatRecordSpec files\n",
    "# String format placeholder {0} should be survey number and {1} should be table id name\n",
    "specFilePatterns = [\n",
    "    r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\DataExtraction\\20150626_FullSiteScrape\\ParsedSpecs\\{0}.*.FlatRecordSpec.csv',\n",
    "    r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\DataExtraction\\20160307_Updates\\ParsedSpecs\\{0}.*.FlatRecordSpec.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Specify master table\n",
    "\n",
    "The output will be in terms of this, i.e. there will be one row for each row of this input table.\n",
    "\n",
    "All other tables must be capable of joining to this table either 1:1 or M:1. Any that join \n",
    "1:M would result in duplicate rows from the left outer join, possibly exponentially many if\n",
    "there are several such tables. The user needs to check this. If the join process seems to be taking a long \n",
    "time or using a lot of memory then this is probably what has gone wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The Reproduction and Birth History table: all children (not just those under 5)\n",
    "masterTable = \"REC21\" #\"RECH1\" #\"REC21\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the tasking - the required output columns\n",
    "\n",
    "i.e. which columns from which tables should be copied into the joined output table\n",
    "\n",
    "We will parse the input tasking file and build a dictionary mapping the name of the input table to a list of the columns (each specified as a ColumnInfo instance) required from that table. We will save this to the notebook global **tableVars**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a dictionary of the columns that have been requested for each table\n",
    "# Key is the table name, and value is a list (because order matters) of the \n",
    "# column names/lengths (each in a 2-item dict).\n",
    "tableVars = defaultdict(list)\n",
    "with open(varsFile) as varfile:\n",
    "    reader = csv.DictReader(varfile, delimiter=',') # delim ; in original\n",
    "    for row in reader:\n",
    "        varname = row['Name']\n",
    "        recname = row['RecordName']\n",
    "        varlength = row['Len']\n",
    "        if 1:#recname in allTables:\n",
    "            tableVars[recname].append(ColumnInfo({\"Name\": varname, \"Length\": varlength}))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to guesstimate the table ID (join) columns\n",
    "\n",
    "Some of the join / identification columns are specified in the .DCF - these are the \"normal\" joins for things like a single-level (e.g. child tables only) extraction. Others aren't and we have to infer them. This can't always be done automatically so the resulting join SQL should always be checked before trusting the results.\n",
    "\n",
    "Call the function after defining it and save the results to a notebook global **tableIdsFull**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTableIDs(surveySpecFile, interestingTables):\n",
    "    '''Pass a table spec file from the DCF parser, and a list of table names to get IDs for.\n",
    "    \n",
    "    We need to figure out the join fields for each table so we can guess, as far as possible,\n",
    "    how to join the tables to one another.\n",
    "    We can't do a perfect job because not everything is always specified in the DCF files, so \n",
    "    for any given extraction schema we must check the SQL that is generated before pulling \n",
    "    the trigger.\n",
    "    \n",
    "    We may wish to extend this to read the relationship spec file too, to pull out\n",
    "    for example HA0 and HC0 as IDs in RECH5/RECH6\n",
    "    \n",
    "    Note that the relative order of the id columns between the tables is important\n",
    "    as it is used by the joiner code to figure out which columns match to which\n",
    "    The fieldnames in the parsed files do give them in a consistent order,\n",
    "    but it might be more relaxing to actually check that here (CASEID first then \n",
    "    BIDX or whatever)\n",
    "    '''\n",
    "    tableIds = defaultdict(list)\n",
    "    with open(surveySpecFile) as specfile:\n",
    "        reader = csv.DictReader(specfile, delimiter = ',')\n",
    "        for row in reader:\n",
    "            recname = row['RecordName']\n",
    "            if (recname in interestingTables and\n",
    "                (row['ItemType'] in ['IdItem', 'JoinableItem']\n",
    "                 )):\n",
    "                # other interesting things include \"line number\" or \"Index\" \n",
    "                # in the row label, but these give false positives.\n",
    "                varname = row['Name']\n",
    "                varlength = row['Len']\n",
    "                if 'IDX' in varname:\n",
    "                    varlength = 2\n",
    "                colInfo = ColumnInfo({\"Name\": varname, \"Length\":varlength})\n",
    "                if colInfo not in tableIds[recname]:\n",
    "                    tableIds[recname].append(colInfo)\n",
    "    return tableIds\n",
    "\n",
    "# Ensure that we are always adding the iditems and joinable column for each table\n",
    "# Haven't used a separate input for the newer surveys\n",
    "fullSpecFile = os.path.join(\n",
    "    r'\\\\map-fs1.ndph.ox.ac.uk\\map_data\\DHS_Automation\\DataExtraction\\20150626_FullSiteScrape',\n",
    "    'SchemaMappingSupport',\n",
    "    'SchemaMapperTableSpecs_AllTables_AllSurveys_InclTypes.csv'\n",
    ")\n",
    "tableIdsFull = getTableIDs(fullSpecFile, tableVars.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If any further manipulation of the parsed information is required then do it here\n",
    "\n",
    "e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tableVars.pop('RECH1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tableIdsFull['REC01'][0].Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tableIdsFull['REC01'].append(ColumnInfo({\"Name\":\"V003\", \"Length\":3}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[c.Name for c in tableIdsFull['REC01']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[c.Name for c in tableIdsFull['RECH1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the list of surveys we need \n",
    "\n",
    "(Or just enter them manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get files to read from survey_db_list\n",
    "with open(svyFile) as svyfile:\n",
    "    reader = csv.DictReader(svyfile)\n",
    "    svys = [row['DHS_id'] for row in reader]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svys.extend([393, 421,239,311,425,437,451,473,457,450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(svys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#svys = [450]\n",
    "#svys = [393, 421,239,311,425,437,451,473,457,450]\n",
    "svys=[248]\n",
    "# or do all that are available\n",
    "# svys= [os.path.basename(f).split('.')[0] \n",
    "#       for f in glob.glob(tblPattern.format(\"*\", \"RECH0\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the surveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is just for development of the table joining code, it will prevent anything actually happening\n",
    "skipDB = False\n",
    "\n",
    "# Should we ignore any table/survey pairs where none of the requested data is present\n",
    "skipBlanks = False\n",
    "\n",
    "# Should we just print out the output-generation SQL for testing\n",
    "isDebug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** For each survey: ***\n",
    "* Create a new in-memory DB\n",
    "* Load all required CSV files for that survey into individual tables in the in-memory DB \n",
    "* Create indexes on the join columns in the DB\n",
    "* Create an output table that is the result of joining them all\n",
    "* Write that table to disk as CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Function for processing one survey ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processSurvey(svyID, tblPatterns, tblVars, allTblIdCols, masterTable, outFile, verbose = True ):\n",
    "    # this creates the new temporary db\n",
    "    db = sqlite3.connect(':memory:')\n",
    "    cursor = db.cursor()\n",
    "    srcTableInfos = {}\n",
    "    \n",
    "    for tblName, tblCols in tblVars.iteritems():\n",
    "        tblIdCols = allTblIdCols[tblName]\n",
    "        # Load one table of this survey into the database\n",
    "        # Find the individual file required\n",
    "        for tblPattern in tblPatterns:\n",
    "            tblFiles = glob.glob(tblPattern.format(svyID, tblName))\n",
    "            if len(tblFiles) > 0:\n",
    "                break\n",
    "        if len(tblFiles) != 1:\n",
    "            print (\"Survey \"+str(svyID)+\" table \"+tblName+\" does not exist or is not well specified!\")\n",
    "            continue\n",
    "        print tblName +\"... \",\n",
    "        tblFile = tblFiles[0]\n",
    "        with open(tblFile) as tbl:\n",
    "            reader = csv.DictReader(tbl)\n",
    "            \n",
    "            # Create a tableinfo object which will handle building the sql necessary\n",
    "            # for interacting with this table in the database\n",
    "            srcTable = TableInfo(tblName, tblIdCols, tblCols)\n",
    "            if (skipDB):\n",
    "                # For debugging of TableInfo\n",
    "                continue\n",
    "                \n",
    "            # Get the sql to create the table in the database\n",
    "            createSql = srcTable.GetCreateTableSQL()\n",
    "            orderedCols = srcTable.AllColumns()\n",
    "            \n",
    "            # Get the \"insert into xx(yy,bb) VALUES...\" part of the SQL to populate the\n",
    "            # data into the DB from the CSV reader\n",
    "            insertSql = srcTable.GetInsertSQLTemplate()\n",
    "            \n",
    "            # Read the data from the CSV file into a list of lists, each correctly \n",
    "            # ordered for the columns that are in the DB table insert statement.\n",
    "            # Use value \"N/A\" for any columns that are not present in this survey\n",
    "            data = [([row.get(i, 'N/A') for i in orderedCols]) for row in reader ]\n",
    "            \n",
    "            # if an incoming CSV table has none of the columns we asked for except IDs\n",
    "            # (e.g. we only wanted some non-standard survey specific columns and this survey \n",
    "            # doesn't have them)\n",
    "            # then don't just include its ID columns, just skip dealing with it altogether\n",
    "            gotData = False\n",
    "            for i in data:\n",
    "                if i.count('N/A') < (len(i) - len(tblIdCols)):\n",
    "                    gotData = True\n",
    "                    break\n",
    "            if skipBlanks and not gotData:\n",
    "                print \"Skipping table {0} as none of the required cols are present\".format(\n",
    "                    tblName)\n",
    "                continue\n",
    "            \n",
    "            # otherwise save the tableinfo\n",
    "            srcTableInfos[tblName] = srcTable\n",
    "            # and create and populate the table into the db\n",
    "            cursor.execute(createSql)\n",
    "            cursor.executemany(insertSql, data)\n",
    "            # and create indexes in the DB on the relevant join columns\n",
    "            idxSql = srcTable.GetCreateIndexSQL()\n",
    "            cursor.executescript(idxSql)\n",
    "        db.commit()\n",
    "    \n",
    "    # Now the in-memory database is populated for this survey we can continue.\n",
    "    # Get a list of all the table names, but with the master table\" \n",
    "    # - i.e. the left one on the left outer join - at the start of \n",
    "    # the list as required by MultiTableJoiner and the rest sorted after\n",
    "    tblNames = [i for i in sorted(srcTableInfos) if i != masterTable]\n",
    "    if masterTable in srcTableInfos:\n",
    "        tblNames[0] = masterTable\n",
    "    else:\n",
    "        print \"Warning: requested master table {0} isn't present! Join may fail!\".format(masterTable)\n",
    "    \n",
    "    if (len(tblNames)) == 0:\n",
    "        print \"Nothing for survey \" + str(svyID)\n",
    "        return\n",
    "    \n",
    "    # Note that we also don't actually check here if the join is appropriate. \n",
    "    # For example from a Child master table we can join to its parents table and the household \n",
    "    # table. But we shouldn't do the reverse as for each household there are many children.\n",
    "    # If we tried, we'd get repeated rows (probably) on the left join.\n",
    "    # If there was more than one such table then we would get an exploding number of rows.\n",
    "    # The table joiner code makes some basic effort to check this based on the number and length\n",
    "    # of join columns.\n",
    "    \n",
    "    # Instantiate the object to write the join SQL\n",
    "    multi = MultiTableJoiner(\"outputTbl\", [srcTableInfos[n] for n in tblNames] )\n",
    "    # and get the SQL. This is just returning the SQL string, not running it.\n",
    "    # Use GetCreateIntoSQL(QualifyFieldNames=True) to name output fields like \n",
    "    # RECH2_HV270 rather than just HV270\n",
    "    joinEmAllSQL = multi.GetCreateIntoSQL(QualifyFieldNames=True)\n",
    "   \n",
    "    # One-off bodge for Donal's data where we want to join the household schedule table \n",
    "    # to the child table. This uses a different join column in the child tables, i.e. normally \n",
    "    # child tables are joined to mother tables based on BIDX, but the child tables also contain a column \n",
    "    # for household-schedule joining called B16. Thanks to that unhelpful name (REC21.B16) there is \n",
    "    # no way that I can see of automatically inferring this from the .DCF specification files.\n",
    "    #joinEmAllSQL = joinEmAllSQL.replace (\n",
    "    #    'LEFT JOIN RECH1 ON substr(REC21.CASEID, 1, length(REC21.CASEID)-3) = RECH1.HHID and REC21.BIDX = RECH1.HVIDX',\n",
    "    #    'LEFT JOIN RECH1 ON substr(REC21.CASEID, 1, length(REC21.CASEID)-3) = RECH1.HHID and REC21.B16 = RECH1.HVIDX'\n",
    "    #)\n",
    "    \n",
    "    if isDebug:\n",
    "        print joinEmAllSQL\n",
    "        return\n",
    "    \n",
    "    # now execute the SQL, thus creating outputTbl in the in-memory database\n",
    "    cursor.execute(joinEmAllSQL)\n",
    "    # and dumpy the results out to CSV\n",
    "    cursor.execute(\"select * from outputTbl\")\n",
    "    colNames = [description[0] for description in cursor.description]\n",
    "    \n",
    "    # TODO a given column should always appear in the same table but occasionally \n",
    "    # this is not the case. So we have to specify in the input file all the places it \n",
    "    # could come from, which will generate multiple columns in the output.\n",
    "    # e.g. some surveys have HV270 in RECH3 rather than RECH2 and so we need to specify \n",
    "    # both if we are running for all surveys.\n",
    "    # Ideally we would check here for these duplicates and write out only the one which \n",
    "    # doesn't have \"N/A\" in the values. But that would need to inspect each row, and thus \n",
    "    # would be much slower. So for now it's best to just use the QualifyFieldNames option \n",
    "    # above to ensure they all get written somehow.\n",
    "    with open(outFile, \"wb\") as f:\n",
    "        writer = UnicodeWriter(f)\n",
    "        writer.writerow(colNames)\n",
    "    #print \"\"\n",
    "        writer.writerows(cursor)\n",
    "    db.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Run the above function for all the surveys ***\n",
    "\n",
    "Here is where we actually do the work and create the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def processSurvey(svyID, tblPatterns, tblVars, tblIdCols, masterTable, outFile, verbose = True ):\n",
    "\n",
    "for svyID in svys:\n",
    "    print \"Survey \"+str(svyID)\n",
    "    outname = outFNPattern.format(svyID)\n",
    "    processSurvey(svyID, tablePatterns, tableVars, tableIdsFull, masterTable, outname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workings below this point - all redundant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on the methodology: the table joiner code creates a new table that is the joined result, i.e. \n",
    "\n",
    "```SQL\n",
    "CREATE TABLE output AS blahblah\n",
    "```\n",
    "\n",
    "Originally the code was written with the intention of first generating an output table as a copy of the relevant parts of the master input table, and then joining each of the other input tables in turn to populate the remaining columns.\n",
    "For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inTable = srcTableInfos['REC43']\n",
    "cp = TableToTableFieldCopier(outTable, inTable, inTable.OutputColumns())\n",
    "\n",
    "update = cp.GetUpdateSQL_Join()\n",
    "cursor.execute(update)\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that doesn't work because it turns out SQLite ***doesn't support join in an update query***. Nice to know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inTable = srcTableInfos['REC43']\n",
    "cp = TableToTableFieldCopier(outTable, inTable, inTable.OutputColumns())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead we might try REPLACE INTO. But that doesn't work because it adds duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inTable = srcTableInfos['REC43']\n",
    "cp = TableToTableFieldCopier(outTable, inTable, inTable.OutputColumns())\n",
    "\n",
    "update = cp.GetUpdateSQL_Replace()\n",
    "cursor.execute(update)\n",
    "db.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So instead we ended up with the one-step approach demonstrated above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
