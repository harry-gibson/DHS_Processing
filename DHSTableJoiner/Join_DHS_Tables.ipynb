{
 "metadata": {
  "name": "",
  "signature": "sha256:014c38e5e9bf8999b357fc58d3640d4f356bda8b7179e978c558ee9184dfea4e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "DHS recode generator"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This file contains code for producing custom output \"recode\" tables from DHS tables that have first been processed out into individual files for each survey and table (record type). \n",
      "\n",
      "Essentially it provides a means of \"joining\" tables that are stored in flat CSV files, just like executing a join query on data held in a DB. The joining is done by loading each of the input tables to an in-memory SQLite database, building the output table there, and dumping the output table back to a CSV file.\n",
      "\n",
      "The code for actually constructing the SQL that builds the in-memory database, and performs the joins, is implemented in an external module that should be in the same directory as this notebook. This notebook just contains the necessary code to read the input requirements and data, and write the output files.\n",
      "\n",
      "This was developed by Harry Gibson for extracting information (potentially) related to Under 5 Mortality for Donal Bisanzio. However it should be applicable for creating any \"flat\" joined output tables from DHS data that has been parsed into separate tables from the CSPro format."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Usage"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The main input is a list of tables / variables that should go into the output, specified in a CSV file. This has one row for each output column, and it should have the columns \"Name\" and \"RecordName\" which specify the variable name and table name respectively.\n",
      "\n",
      "A second input CSV file provides a list of survey IDs (in a \"DHS_id\" column) that data should be extracted from.\n",
      "\n",
      "Finally a directory of the parsed DHS survey tables (produced using the DHS survey parsing code) must be provided. The survey id and record name from the above inputs will be used to select CSV files from this directory."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "import glob\n",
      "from collections import defaultdict\n",
      "import sqlite3\n",
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from DHSTableManagement import *\n",
      "from UnicodeWriter import UnicodeWriter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Specify input file locations / patterns"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "varsFile = r'\\\\129.67.26.176\\map_data\\DHS_Automation\\Processing\\U5M_TheUniverse_And_Everything_201510\\Info\\variables_chosen_edits.csv'\n",
      "svyFile = r'\\\\129.67.26.176\\map_data\\DHS_Automation\\Processing\\U5M_TheUniverse_And_Everything_201510\\Info\\survey_db_list.csv'\n",
      "tblPattern = r'\\\\129.67.26.176\\map_data\\DHS_Automation\\DataExtraction\\20150626_FullSiteScrape\\ProcessedTables\\{0}.*.{1}.csv'\n",
      "outputFilenameTag = \"U5M_Data_Repeat\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#varsFile = r'\\\\129.67.26.176\\map_data\\DHS_Automation\\Processing\\HouseholdElectricity_201511\\Info\\variables_for_hh_electric.csv'\n",
      "#svyFile = r'\\\\129.67.26.176\\map_data\\DHS_Automation\\Processing\\U5M_Universe_And_Everything_201510\\Info\\survey_db_list.csv'\n",
      "#tblPattern = r'\\\\129.67.26.176\\map_data\\DHS_Automation\\DataExtraction\\20150626_FullSiteScrape\\ProcessedTables\\{0}.*.{1}.csv'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outDir = r'\\\\129.67.26.176\\map_data\\hsg\\Donal'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#outDir = r'\\\\129.67.26.176\\map_data\\DHS_Automation\\Processing\\HouseholdElectricity_201511\\Out'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outFNPattern = os.path.join(outDir,outputFilenameTag+\".{0!s}.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# allTables = [\"REC01\",\"REC11\",\"REC21\",\"REC22\",\"REC41\",\"REC42\",\"REC43\",\"REC44\",\"REC51\",\n",
      "#             \"REC71\",\"REC75\",\"REC91\",\"REC94\",\"REC95\",\"RECH0\",\"RECH1\",\"RECH2\",\"RECH3\",\n",
      "#             \"RECH4\",\"RECHM2\"]\n",
      "# \"RECH5\",\"RECH6\", missed as they contain nothing new"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Specify master table"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# All other tables must be capable of joining to this table either 1:1 or M:1. Any that join \n",
      "# 1:M would result in duplicate rows from the left outer join, possibly exponentially many if\n",
      "# there are several such tables.\n",
      "masterTable = \"REC21\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Build a dictionary of the columns that have been requested for each table\n",
      "# Key is the tablename and value is a list of the column names.\n",
      "tableVars = defaultdict(list)\n",
      "with open(varsFile) as varfile:\n",
      "    reader = csv.DictReader(varfile,delimiter=',') # delim ; in original\n",
      "    for row in reader:\n",
      "        varname = row['Name']\n",
      "        recname = row['RecordName']\n",
      "        if 1:#recname in allTables:\n",
      "            tableVars[recname].append(varname)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get files to read from survey_db_list\n",
      "with open(svyFile) as svyfile:\n",
      "    reader = csv.DictReader(svyfile)\n",
      "    svys = [row['DHS_id'] for row in reader]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# or do all that are available\n",
      "# svys= [os.path.basename(f).split('.')[0] \n",
      "#       for f in glob.glob(tblPattern.format(\"*\", \"RECH0\"))]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Process the surveys"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "skipDB = False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Load all CSV files for a survey into individual tables in the in-memory DB \n",
      "* Create indexes\n",
      "* Create an output table that is the result of joining them all\n",
      "* Write that to disk."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for svyID in svys:\n",
      "    print \"Survey \"+str(svyID)\n",
      "    # Use an in-memory sqlite db to load the tables and join them \n",
      "    db = sqlite3.connect(':memory:')\n",
      "    cursor = db.cursor()\n",
      "    srcTableInfos = {}\n",
      "    outname = outFNPattern.format(svyID)\n",
      "    for tblID, tblCols in tableVars.iteritems():\n",
      "        # Load one table into the database. \n",
      "        \n",
      "        # Find the individual file required\n",
      "        tblFiles = glob.glob(tblPattern.format(svyID, tblID))\n",
      "        if len(tblFiles) != 1:\n",
      "            print (\"Survey \"+str(svyID)+\" table \"+tblID+\" does not exist or is not well specified!\")\n",
      "            continue\n",
      "        print tblID +\"... \",\n",
      "        tblFile = tblFiles[0]\n",
      "        \n",
      "        # build a TableInfo for working with the file and use it to load the data to a DB table\n",
      "        with open(tblFile) as tbl:\n",
      "            reader = csv.DictReader(tbl)\n",
      "            # Ensure we get the ID (join) variables, which are CASEID or HIDX / BIDX etc,\n",
      "            # regardless of what is specified in the output spec file.\n",
      "            \n",
      "            # Note that there are also HA0 and HC0 as IDs in RECH5/RECH6 but we don't \n",
      "            # actually need those tables (The only cols so far requested from those tables \n",
      "            # are duplicated in the woman tables).\n",
      "            # So for now we'll just look for any columns with ID in the name. To do it \n",
      "            # \"properly\" we would have to go back to the DCF parsing code and pull out the \n",
      "            # relationship info there.\n",
      "            # Note that the relative order of the id columns between the tables is important\n",
      "            # as it is used by the joiner code to figure out which columns match to which\n",
      "            # The fieldnames in the parsed files do give them in a consistent order,\n",
      "            # but it might be more relaxing to actually check that here (CASEID first then \n",
      "            # BIDX or whatever)\n",
      "            ids = [v for v in reader.fieldnames if v.find(\"ID\") != -1]\n",
      "            for i in ids:\n",
      "                if i not in tblCols:\n",
      "                    tblCols.insert(0, i)\n",
      "            # Create a tableinfo object which will handle building the sql necessary\n",
      "            # for interacting with this table in the database\n",
      "            srcTable = TableInfo(tblID, ids, tblCols)\n",
      "            \n",
      "            if (skipDB):\n",
      "                # For debugging of TableInfo\n",
      "                continue\n",
      "\n",
      "            # Get the sql to create the table in the database\n",
      "            createSql = srcTable.GetCreateTableSQL()\n",
      "            orderedCols = srcTable.AllColumns()\n",
      "            \n",
      "            # Populate the data into the DB from the CSV reader\n",
      "            insertSql = srcTable.GetInsertSQLTemplate()\n",
      "            # Use \"N/A\" for any columns that are not present in this survey\n",
      "            data = [([row.get(i, 'N/A') for i in orderedCols]) for row in reader ]\n",
      "            \n",
      "            # if a table doesn't have any of the columns we asked for then don't just \n",
      "            # include its ID columns, just skip dealing with it altogether\n",
      "            gotData = False\n",
      "            for i in data:\n",
      "                if i.count('N/A') < (len(i) - len(ids)):\n",
      "                    gotData = True\n",
      "                    break\n",
      "            if not gotData:\n",
      "                print \"Skipping table {0} as none of the required cols are present\".format(\n",
      "                    tblID)\n",
      "                continue\n",
      "                \n",
      "            # otherwise save the tableinfo\n",
      "            srcTableInfos[tblID] = srcTable\n",
      "            # and put the data into the db\n",
      "            cursor.execute(createSql)\n",
      "            cursor.executemany(insertSql, data)\n",
      "            # and create indexes in the DB on the relevant join columns\n",
      "            idxSql = srcTable.GetCreateIndexSQL()\n",
      "            cursor.executescript(idxSql)\n",
      "        db.commit()\n",
      "        \n",
      "    # Move the \"master table\" - i.e. the left one on the left outer join - \n",
      "    # to the start of the list as required by MultiTableJoiner\n",
      "    tblNames = [i for i in sorted(srcTableInfos) if i != masterTable]\n",
      "    if masterTable in srcTableInfos:\n",
      "        tblNames[0] = masterTable\n",
      "    else:\n",
      "        print \"Warning: requested master table {0} isn't present! Join may fail!\"\n",
      "    # Note that we also don't actually check here if the join is appropriate. \n",
      "    # For example from a Child master table we can join to its parents table and the household \n",
      "    # table. But we couldn't do the reverse as for each household there are many children.\n",
      "    # If we tried, we'd get repeated rows (probably) on the left join.\n",
      "    # If there was more than one such table then we would get an exploding number of rows.\n",
      " \n",
      "    if (len(tblNames)) == 0:\n",
      "        print \"Nothing for survey \" + str(svyID)\n",
      "        continue\n",
      "    \n",
      "    # Perform the join!\n",
      "    multi = MultiTableJoiner(\"outputTbl\", [srcTableInfos[n] for n in tblNames] )\n",
      "    # Use GetCreateIntoSQL(QualifyFieldNames=True) to name output fields like \n",
      "    # RECH2_HV270 rather than just HV270\n",
      "    cursor.execute(multi.GetCreateIntoSQL())\n",
      "    \n",
      "    # Write the results out to CSV\n",
      "    cursor.execute(\"select * from outputTbl\")\n",
      "    colNames = [description[0] for description in cursor.description]\n",
      "    # TODO a given column should always appear in the same table but occasionally \n",
      "    # this is not the case. So we have to specify in the input file all the places it \n",
      "    # could come from, which will generate multiple columns in the output.\n",
      "    # e.g. some surveys have HV270 in RECH3 rather than RECH2 and so we need to specify \n",
      "    # both if we are running for all surveys.\n",
      "    # Ideally we would check here for these duplicates and write out only the one which \n",
      "    # doesn't have \"N/A\" in the values. But that would need to inspect each row, and thus \n",
      "    # would be much slower.\n",
      "    with open(outname, \"wb\") as f:\n",
      "        writer = UnicodeWriter(f)\n",
      "        writer.writerow(colNames)\n",
      "    #print \"\"\n",
      "        writer.writerows(cursor)\n",
      "    db.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Survey 216\n",
        "Survey 216 table  does not exist!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Survey 216 table RECML does not exist!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "REC51... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " REC11...  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "REC91...  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "REC94...  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "REC95...  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "REC75...  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "REC71...  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RECH4...  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Skipping table RECH4 as none of the required cols are present\n",
        "RECH3... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " RECH2...  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RECH1...  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RECH0...  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "REC21...  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "REC22...  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "REC01...  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "REC43...  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "REC42...  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "REC41...  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "REC44...  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Survey 216 table RECHM2 does not exist!\n",
        "410\n",
        "410\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Workings below this point - all redundant"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the best thing to do would be then to join each of the other tables in turn and update, for example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inTable = srcTableInfos['REC43']\n",
      "cp = TableToTableFieldCopier(outTable, inTable, inTable.OutputColumns())\n",
      "\n",
      "update = cp.GetUpdateSQL_Join()\n",
      "cursor.execute(update)\n",
      "db.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "OperationalError",
       "evalue": "near \"INNER\": syntax error",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-90-7392b406013b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mupdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetUpdateSQL_Join\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# UPDATE {0} o INNER JOIN {1} i on o.{2} = i.{3} SET o.{4} = i.{5};\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mOperationalError\u001b[0m: near \"INNER\": syntax error"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But that doesn't work because it turns out SQLite doesn't support join in an update query. Nice to know."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inTable = srcTableInfos['REC43']\n",
      "cp = TableToTableFieldCopier(outTable, inTable, inTable.OutputColumns())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 101
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Instead we might try REPLACE INTO. But that doesn't work because it adds duplicate rows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inTable = srcTableInfos['REC43']\n",
      "cp = TableToTableFieldCopier(outTable, inTable, inTable.OutputColumns())\n",
      "\n",
      "update = cp.GetUpdateSQL_Replace()\n",
      "cursor.execute(update)\n",
      "db.commit()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}